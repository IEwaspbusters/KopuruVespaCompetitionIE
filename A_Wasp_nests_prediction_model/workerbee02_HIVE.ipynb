{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nkDv5dppU6B"
   },
   "source": [
    "# HIVE algorithm **Kopuru Vespa Velutina Competition**\n",
    "\n",
    "Purpose: to process the weather data from Biscay's weather stations into a workable dataset.\n",
    "\n",
    "Output: METEO dataset *(WBds02_METEO.csv)*\n",
    "\n",
    "@authors:\n",
    "* mario.bejar@student.ie.edu\n",
    "* pedro.geirinhas@student.ie.edu\n",
    "* a.berrizbeitia@student.ie.edu\n",
    "* pcasaverde@student.ie.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.20.2'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.version.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'../Input_open_data' # use your path\n",
    "files = glob.glob(path + \"/*.csv\") #We create a list with all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We filter meteo files\n",
    "files_meteo=[]\n",
    "\n",
    "for csv in files:\n",
    "    if csv.find(\"2019\")  != -1 or csv.find(\"2018\")  != -1 or csv.find(\"2017\") != -1or  csv.find(\"2016\") != -1:\n",
    "        files_meteo.append(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataframe\n",
    "df = pd.concat([pd.read_csv(fp, header=None, sep=';').assign(new=os.path.basename(fp).split('.')[0]) for fp in files_meteo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning\n",
    "df.rename(columns=df.iloc[0], inplace=True)\n",
    "df.columns\n",
    "df.rename(columns={'ds06-2016_DÍAS DE HELADA 2016': 'new'}, inplace= True)\n",
    "df=df.loc[~df['COD.'].isin(['KOD.','COD.' ]),:].dropna(subset=['COD.']).drop(columns=[\"cota (m)\", \"SUMA\"])\n",
    "\n",
    "# Extract year from the string  \n",
    "df['year'] = df['new'].str.extract('(\\d\\d\\d\\d)', expand=True)\n",
    "\n",
    "#Función para crear codigo_merge\n",
    "def str_join(df, sep, *cols):\n",
    "    from functools import reduce\n",
    "    return reduce(lambda x, y: x.astype(str).str.cat(y.astype(str), sep=sep), \n",
    "                 [df[col] for col in cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new'] = df['new'].str.slice(10, 100) #Cleaning the name of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Get variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alebe\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "#Variables------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Freeze----------------------------\n",
    "freez= df[df['new'].str.contains(\"HELADA\")].drop(columns=['new'])\n",
    "freez=pd.melt(freez, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='freez')\n",
    "\n",
    "freez['merge_cod'] = str_join(freez,'_' , 'COD.','ESTACION','year', 'month')\n",
    "freez.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "cols= ['merge_cod', 'freez']\n",
    "freez= freez.reindex(columns= cols)\n",
    "\n",
    "## Rain ------------------------------\n",
    "rain= df[df['new'].str.contains(\"DÍAS DE PRECIPITACIÓN 20\")].drop(columns=['new'])\n",
    "rain=pd.melt(rain, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='rain')\n",
    "\n",
    "rain['merge_cod'] = str_join(rain,'_' , 'COD.','ESTACION','year', 'month')\n",
    "\n",
    "rain.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## rain_1mm----------------------------\n",
    "\n",
    "rain_1mm= df[df['new'].str.contains(\"DÍAS DE PRECIPITACIÓN IGUAL O SUPERIOR\")].drop(columns=['new'])\n",
    "rain_1mm=pd.melt(rain_1mm, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='rain_1mm')\n",
    "\n",
    "rain_1mm['merge_cod'] = str_join(rain_1mm,'_' , 'COD.','ESTACION','year', 'month')\n",
    "rain_1mm.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## rain_cum------------------------------\n",
    "rain_cum= df[df['new'].str.contains(\"PRECIPITACIÓN ACUMULADA\")].drop(columns=['new'])\n",
    "rain_cum=pd.melt(rain_cum, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='rain_cum')\n",
    "\n",
    "rain_cum['merge_cod'] = str_join(rain_cum,'_' , 'COD.','ESTACION','year', 'month')\n",
    "rain_cum.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## rain_max_10------------------------------\n",
    "rain_max_10= df[df['new'].str.contains(\"PRECIPITACIÓN MÁXIMA EN 10 MINUTOS\")].drop(columns=['new'])\n",
    "rain_max_10=pd.melt(rain_max_10, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='rain_max_10')\n",
    "\n",
    "rain_max_10['merge_cod'] = str_join(rain_max_10,'_' , 'COD.','ESTACION','year', 'month')\n",
    "rain_max_10.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## rain_max_day------------------------------\n",
    "rain_max_day= df[df['new'].str.contains(\"PRECIPITACIÓN MÁXIMA EN UN DÍA\")].drop(columns=['new'])\n",
    "rain_max_day=pd.melt(rain_max_day, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='rain_max_day')\n",
    "\n",
    "rain_max_day['merge_cod'] = str_join(rain_max_day,'_' , 'COD.','ESTACION','year', 'month')\n",
    "rain_max_day.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## hum------------------------------\n",
    "hum= df[df['new'].str.contains(\"HUMEDAD MEDIA\")].drop(columns=['new'])\n",
    "hum=pd.melt(hum, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='hum')\n",
    "\n",
    "hum['merge_cod'] = str_join(hum,'_' , 'COD.','ESTACION','year', 'month')\n",
    "hum.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## sun------------------------------\n",
    "sun= df[df['new'].str.contains(\"IRRADIACIÓN MEDIA\")].drop(columns=['new'])\n",
    "sun=pd.melt(sun, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='sun')\n",
    "\n",
    "sun['merge_cod'] = str_join(sun,'_' , 'COD.','ESTACION','year', 'month')\n",
    "sun.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## lev_max------------------------------\n",
    "lev_max= df[df['new'].str.contains(\"NIVEL MÁXIMO\")].drop(columns=['new'])\n",
    "lev_max=pd.melt(lev_max, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='lev_max')\n",
    "\n",
    "lev_max['merge_cod'] = str_join(lev_max,'_' , 'COD.','ESTACION','year', 'month')\n",
    "lev_max.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "## lev_mid------------------------------\n",
    "lev_mid= df[df['new'].str.contains(\"NIVEL MEDIO\")].drop(columns=['new'])\n",
    "lev_mid=pd.melt(lev_mid, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='lev_mid')\n",
    "\n",
    "lev_mid['merge_cod'] = str_join(lev_mid,'_' , 'COD.','ESTACION','year', 'month')\n",
    "lev_mid.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## lev_min------------------------------only 2019\n",
    "lev_min= df[df['new'].str.contains(\"NIVEL MÍNIMO\")].drop(columns=['new'])\n",
    "lev_min=pd.melt(lev_min, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='lev_min')\n",
    "\n",
    "lev_min['merge_cod'] = str_join(lev_min,'_' , 'COD.','ESTACION','year', 'month')\n",
    "lev_min.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## temp_max_abs------------------------------\n",
    "temp_max_abs= df[df['new'].str.contains(\"TEMPERATURA MÁXIMA ABSOLUTA\")].drop(columns=['new'])\n",
    "temp_max_abs=pd.melt(temp_max_abs, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='temp_max_abs')\n",
    "\n",
    "temp_max_abs['merge_cod'] = str_join(temp_max_abs,'_' , 'COD.','ESTACION','year', 'month')\n",
    "temp_max_abs.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## temp_max_avg-----------------------------\n",
    "temp_max_avg= df[df['new'].str.contains(\"TEMPERATURA MÁXIMA MEDIA\")].drop(columns=['new'])\n",
    "temp_max_avg=pd.melt(temp_max_avg, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='temp_max_avg')\n",
    "\n",
    "temp_max_avg['merge_cod'] = str_join(temp_max_avg,'_' , 'COD.','ESTACION','year', 'month')\n",
    "temp_max_avg.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## temp_avg----------------------------\n",
    "temp_avg= df[df['new'].str.contains(\"TEMPERATURA MEDIA\")].drop(columns=['new'])\n",
    "temp_avg=pd.melt(temp_avg, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='temp_avg')\n",
    "\n",
    "temp_avg['merge_cod'] = str_join(temp_avg,'_' , 'COD.','ESTACION','year', 'month')\n",
    "temp_avg.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## temp_min_abs----------------------------\n",
    "temp_min_abs= df[df['new'].str.contains(\"TEMPERATURA MÍNIMA ABSOLUTA\")].drop(columns=['new'])\n",
    "temp_min_abs=pd.melt(temp_min_abs, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='temp_min_abs')\n",
    "\n",
    "temp_min_abs['merge_cod'] = str_join(temp_min_abs,'_' , 'COD.','ESTACION','year', 'month')\n",
    "temp_min_abs.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## wind_max----------------------------\n",
    "wind_max= df[df['new'].str.contains(\"VELOCIDAD DE LA RACHA MÁXIMA\")].drop(columns=['new'])\n",
    "wind_max=pd.melt(wind_max, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='wind_max')\n",
    "\n",
    "wind_max['merge_cod'] = str_join(wind_max,'_' , 'COD.','ESTACION','year', 'month')\n",
    "wind_max.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "\n",
    "## wind_avg----------------------------\n",
    "wind_avg= df[df['new'].str.contains(\"VELOCIDAD MEDIA\")].drop(columns=['new'])\n",
    "wind_avg=pd.melt(wind_avg, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='wind_avg')\n",
    "\n",
    "wind_avg['merge_cod'] = str_join(wind_avg,'_' , 'COD.','ESTACION','year', 'month')\n",
    "wind_avg.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## wind_max_avg----------------------------\n",
    "wind_max_avg= df[df['new'].str.contains(\"MEDIA DE LAS VELOCIDADES MÁXIMAS\")].drop(columns=['new'])\n",
    "wind_max_avg=pd.melt(wind_max_avg, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='wind_max_avg')\n",
    "\n",
    "wind_max_avg['merge_cod'] = str_join(wind_max_avg,'_' , 'COD.','ESTACION','year', 'month')\n",
    "wind_max_avg.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n",
    "\n",
    "## temp_min_avg\n",
    "\n",
    "temp_min_avg= df[df['new'].str.contains(\"TEMPERATURA MÍNIMA MEDIA\")].drop(columns=['new'])\n",
    "temp_min_avg=pd.melt(temp_min_avg, id_vars=['COD.', 'ESTACION', 'year'], value_vars=['ENE', 'FEB', 'MAR', 'ABR', 'MAY',\n",
    "                                                                'JUN', 'JUL', 'AGO', 'SET', 'OCT', 'NOV','DIC'], var_name='month',value_name='temp_min_avg')\n",
    "\n",
    "temp_min_avg['merge_cod'] = str_join(temp_min_avg,'_' , 'COD.','ESTACION','year', 'month')\n",
    "temp_min_avg.drop(columns= ['COD.', 'ESTACION', 'year', 'month'], inplace= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Merge the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_data= freez.merge(hum, on='merge_cod', how= 'outer' ).merge(\n",
    "    lev_max, on='merge_cod', how= 'outer' ).merge(\n",
    "    lev_mid, on='merge_cod', how= 'outer' ).merge(\n",
    "    lev_min, on='merge_cod', how= 'outer' ).merge(\n",
    "    rain, on='merge_cod', how= 'outer' ).merge(\n",
    "    rain_1mm, on='merge_cod', how= 'outer' ).merge(\n",
    "    rain_cum, on='merge_cod', how= 'outer' ).merge(\n",
    "    rain_max_10, on='merge_cod', how= 'outer' ).merge(\n",
    "    rain_max_day, on='merge_cod', how= 'outer' ).merge(\n",
    "    sun, on='merge_cod', how= 'outer' ).merge(\n",
    "    temp_avg, on='merge_cod', how= 'outer' ).merge(\n",
    "    temp_max_abs, on='merge_cod', how= 'outer' ).merge(\n",
    "    temp_max_avg, on='merge_cod', how= 'outer' ).merge(\n",
    "    temp_min_abs, on='merge_cod', how= 'outer' ).merge(\n",
    "    wind_avg, on='merge_cod', how= 'outer' ).merge(\n",
    "    wind_max, on='merge_cod', how= 'outer' ).merge(\n",
    "    wind_max_avg, on='merge_cod', how= 'outer' ).merge(\n",
    "    temp_min_avg, on='merge_cod', how= 'outer' )\n",
    "\n",
    "m_data['code_merge']= m_data['merge_cod']\n",
    "m_data[['codigo',' estacion','year', 'month']]= m_data.merge_cod.str.split(\"_\",expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5040, 25)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m_data.to_csv (r'estaciones.csv', index = False, header=True)\n",
    "\n",
    "#fruta\n",
    "fruta= pd.read_csv('../Input_open_data/ds04_FRUTALES-DECLARADOS-KOPURU.csv', sep=';')\n",
    "#fruta.to_csv (r'fruta.csv', index = False, header=True)\n",
    "\n",
    "#met\n",
    "met= pd.read_csv('../Input_open_data/ds05_LOCALIZACION-ESTACIONES-METEOROLOGICAS.csv', sep=';')\n",
    "#met.to_csv (r'met.csv', index = False, header=True)\n",
    "\n",
    "#apicu\n",
    "apicu= pd.read_csv('../Input_open_data/ds03_APICULTURA_COLMENAS_KOPURU.csv', sep=';')\n",
    "#apicu.to_csv (r'apicu.csv', index = False, header=True)\n",
    "\n",
    "#nido\n",
    "nido=pd.read_csv('../Input_open_data/ds02_datos-nidos-avispa-asiatica.csv')\n",
    "#nido.to_csv (r'nido.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datawig import SimpleImputer\n",
    "from datawig.utils import random_split\n",
    "from sklearn.metrics import f1_score, classification_report, precision_score, recall_score, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['merge_cod', 'freez', 'hum', 'lev_max', 'lev_mid', 'lev_min', 'rain',\n",
       "       'rain_1mm', 'rain_cum', 'rain_max_10', 'rain_max_day', 'sun',\n",
       "       'temp_avg', 'temp_max_abs', 'temp_max_avg', 'temp_min_abs', 'wind_avg',\n",
       "       'wind_max', 'wind_max_avg', 'temp_min_avg', 'code_merge', 'codigo',\n",
       "       ' estacion', 'year', 'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seasons= pd.read_csv('D:/Bootcamp/Data/estaciones.csv')\n",
    "seasons = m_data.copy()\n",
    "seasons.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Impute the NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-14 16:58:17,612 [INFO]  CategoricalEncoder for column hum                                found only 30 occurrences of value 82.2\n",
      "2021-06-14 16:58:17,614 [INFO]  CategoricalEncoder for column hum                                found only 26 occurrences of value 84.2\n",
      "2021-06-14 16:58:17,615 [INFO]  CategoricalEncoder for column hum                                found only 26 occurrences of value 82.8\n",
      "2021-06-14 16:58:17,616 [INFO]  CategoricalEncoder for column hum                                found only 26 occurrences of value 83.1\n",
      "2021-06-14 16:58:17,617 [INFO]  CategoricalEncoder for column hum                                found only 25 occurrences of value 79.0\n",
      "2021-06-14 16:58:17,618 [INFO]  CategoricalEncoder for column hum                                found only 25 occurrences of value 81.4\n",
      "2021-06-14 16:58:17,619 [INFO]  CategoricalEncoder for column hum                                found only 24 occurrences of value 82.1\n",
      "2021-06-14 16:58:17,620 [INFO]  CategoricalEncoder for column hum                                found only 24 occurrences of value 83.6\n",
      "2021-06-14 16:58:17,621 [INFO]  CategoricalEncoder for column hum                                found only 23 occurrences of value 79.8\n",
      "2021-06-14 16:58:17,622 [INFO]  CategoricalEncoder for column hum                                found only 23 occurrences of value 80.2\n",
      "2021-06-14 16:58:17,623 [INFO]  CategoricalEncoder for column hum                                found only 23 occurrences of value 85.2\n",
      "2021-06-14 16:58:17,624 [INFO]  CategoricalEncoder for column hum                                found only 23 occurrences of value 80.6\n",
      "2021-06-14 16:58:17,625 [INFO]  CategoricalEncoder for column hum                                found only 23 occurrences of value 86.4\n",
      "2021-06-14 16:58:17,626 [INFO]  CategoricalEncoder for column hum                                found only 23 occurrences of value 80.9\n",
      "2021-06-14 16:58:17,627 [INFO]  CategoricalEncoder for column hum                                found only 23 occurrences of value 83.3\n",
      "2021-06-14 16:58:17,627 [INFO]  CategoricalEncoder for column hum                                found only 22 occurrences of value 84.4\n",
      "2021-06-14 16:58:17,628 [INFO]  CategoricalEncoder for column hum                                found only 22 occurrences of value 80.3\n",
      "2021-06-14 16:58:17,629 [INFO]  CategoricalEncoder for column hum                                found only 22 occurrences of value 78.0\n",
      "2021-06-14 16:58:17,631 [INFO]  CategoricalEncoder for column hum                                found only 22 occurrences of value 78.9\n",
      "2021-06-14 16:58:17,633 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 85.7\n",
      "2021-06-14 16:58:17,634 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 79.5\n",
      "2021-06-14 16:58:17,635 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 83.5\n",
      "2021-06-14 16:58:17,636 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 81.6\n",
      "2021-06-14 16:58:17,636 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 82.3\n",
      "2021-06-14 16:58:17,637 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 78.6\n",
      "2021-06-14 16:58:17,638 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 74.9\n",
      "2021-06-14 16:58:17,639 [INFO]  CategoricalEncoder for column hum                                found only 21 occurrences of value 85.6\n",
      "2021-06-14 16:58:17,640 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 75.3\n",
      "2021-06-14 16:58:17,640 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 81.9\n",
      "2021-06-14 16:58:17,642 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 76.1\n",
      "2021-06-14 16:58:17,644 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 83.8\n",
      "2021-06-14 16:58:17,645 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 78.4\n",
      "2021-06-14 16:58:17,646 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 82.7\n",
      "2021-06-14 16:58:17,647 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 77.5\n",
      "2021-06-14 16:58:17,649 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 82.5\n",
      "2021-06-14 16:58:17,650 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 81.7\n",
      "2021-06-14 16:58:17,650 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 84.9\n",
      "2021-06-14 16:58:17,652 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 80.5\n",
      "2021-06-14 16:58:17,652 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 83.0\n",
      "2021-06-14 16:58:17,653 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 76.5\n",
      "2021-06-14 16:58:17,654 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 84.1\n",
      "2021-06-14 16:58:17,655 [INFO]  CategoricalEncoder for column hum                                found only 20 occurrences of value 75.5\n",
      "2021-06-14 16:58:17,655 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 78.8\n",
      "2021-06-14 16:58:17,656 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 77.7\n",
      "2021-06-14 16:58:17,657 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 80.1\n",
      "2021-06-14 16:58:17,658 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 77.2\n",
      "2021-06-14 16:58:17,659 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 86.1\n",
      "2021-06-14 16:58:17,660 [INFO]  CategoricalEncoder for column hum                                found only 19 occurrences of value 77.6\n",
      "2021-06-14 16:58:17,661 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 80.8\n",
      "2021-06-14 16:58:17,661 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 86.7\n",
      "2021-06-14 16:58:17,662 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 74.7\n",
      "2021-06-14 16:58:17,663 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 85.0\n",
      "2021-06-14 16:58:17,664 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 79.9\n",
      "2021-06-14 16:58:17,665 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 82.9\n",
      "2021-06-14 16:58:17,666 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 86.2\n",
      "2021-06-14 16:58:17,667 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 76.0\n",
      "2021-06-14 16:58:17,668 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 75.6\n",
      "2021-06-14 16:58:17,669 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 81.8\n",
      "2021-06-14 16:58:17,670 [INFO]  CategoricalEncoder for column hum                                found only 18 occurrences of value 81.5\n",
      "2021-06-14 16:58:17,670 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 80.0\n",
      "2021-06-14 16:58:17,671 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 74.1\n",
      "2021-06-14 16:58:17,672 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 79.7\n",
      "2021-06-14 16:58:17,673 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 72.7\n",
      "2021-06-14 16:58:17,674 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 76.2\n",
      "2021-06-14 16:58:17,676 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 77.9\n",
      "2021-06-14 16:58:17,678 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 80.7\n",
      "2021-06-14 16:58:17,679 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 82.0\n",
      "2021-06-14 16:58:17,680 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 79.3\n",
      "2021-06-14 16:58:17,681 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 81.2\n",
      "2021-06-14 16:58:17,682 [INFO]  CategoricalEncoder for column hum                                found only 17 occurrences of value 85.4\n",
      "2021-06-14 16:58:17,683 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 83.2\n",
      "2021-06-14 16:58:17,684 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 73.0\n",
      "2021-06-14 16:58:17,685 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 77.0\n",
      "2021-06-14 16:58:17,686 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 75.4\n",
      "2021-06-14 16:58:17,686 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 86.6\n",
      "2021-06-14 16:58:17,687 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 78.2\n",
      "2021-06-14 16:58:17,688 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 85.3\n",
      "2021-06-14 16:58:17,689 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 78.7\n",
      "2021-06-14 16:58:17,690 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 83.7\n",
      "2021-06-14 16:58:17,690 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 78.5\n",
      "2021-06-14 16:58:17,691 [INFO]  CategoricalEncoder for column hum                                found only 16 occurrences of value 81.0\n",
      "2021-06-14 16:58:17,692 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 73.9\n",
      "2021-06-14 16:58:17,694 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 87.8\n",
      "2021-06-14 16:58:17,695 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 77.1\n",
      "2021-06-14 16:58:17,696 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 88.0\n",
      "2021-06-14 16:58:17,696 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 88.1\n",
      "2021-06-14 16:58:17,697 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 77.3\n",
      "2021-06-14 16:58:17,698 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 88.8\n",
      "2021-06-14 16:58:17,699 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 84.5\n",
      "2021-06-14 16:58:17,700 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 86.9\n",
      "2021-06-14 16:58:17,700 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 74.8\n",
      "2021-06-14 16:58:17,701 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 86.5\n",
      "2021-06-14 16:58:17,702 [INFO]  CategoricalEncoder for column hum                                found only 15 occurrences of value 79.4\n",
      "2021-06-14 16:58:17,703 [INFO]  CategoricalEncoder for column hum                                found only 14 occurrences of value 84.8\n",
      "2021-06-14 16:58:17,704 [INFO]  CategoricalEncoder for column hum                                found only 14 occurrences of value 76.4\n",
      "2021-06-14 16:58:17,704 [INFO]  CategoricalEncoder for column hum                                found only 14 occurrences of value 82.6\n",
      "2021-06-14 16:58:17,705 [INFO]  CategoricalEncoder for column hum                                found only 14 occurrences of value 76.9\n",
      "2021-06-14 16:58:17,706 [INFO]  CategoricalEncoder for column hum                                found only 14 occurrences of value 84.6\n",
      "2021-06-14 16:58:17,707 [INFO]  CategoricalEncoder for column hum                                found only 14 occurrences of value 79.2\n",
      "2021-06-14 16:58:17,708 [INFO]  CategoricalEncoder for column hum                                found only 14 occurrences of value 75.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-e5793311f259>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#Fit an imputer model on the train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mimputer_hum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mpredictions_hum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimputer_hum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datawig\\simple_imputer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_df, test_df, ctx, learning_rate, num_epochs, patience, test_split, weight_decay, batch_size, final_fc_hidden_units, calibrate, class_weights, instance_weights)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         self.imputer = self.imputer.fit(train_df, test_df, ctx, learning_rate, num_epochs, patience,\n\u001b[0m\u001b[0;32m    387\u001b[0m                                         \u001b[0mtest_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                                         \u001b[0mweight_decay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datawig\\imputer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_df, test_df, ctx, learning_rate, num_epochs, patience, test_split, weight_decay, batch_size, final_fc_hidden_units, calibrate)\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0miter_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__build_iterators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__check_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datawig\\imputer.py\u001b[0m in \u001b[0;36m__build_iterators\u001b[1;34m(self, train_df, test_df, test_split)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Building Train Iterator with {} elements\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         iter_train = ImputerIterDf(\n\u001b[0m\u001b[0;32m    593\u001b[0m             \u001b[0mdata_frame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m             \u001b[0mdata_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_encoders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datawig\\iterators.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_frame, data_columns, label_columns, batch_size)\u001b[0m\n\u001b[0;32m    238\u001b[0m                 \u001b[0mcolumn_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmxnet_iterator_from_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_provide_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprovide_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datawig\\iterators.py\u001b[0m in \u001b[0;36mmxnet_iterator_from_df\u001b[1;34m(self, data_frame)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcol_enc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mdata_array_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_column\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_array_numpy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             logger.debug(\"Data Encoding - Encoded {} rows of column \\\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datawig\\column_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, data_frame)\u001b[0m\n\u001b[0;32m    607\u001b[0m             \u001b[0mtmp_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_columns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m             \u001b[0mtmp_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefixed_concatenation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    312\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m    710\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_1d_arraylike_from_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mconstruct_1d_arraylike_from_scalar\u001b[1;34m(value, length, dtype)\u001b[0m\n\u001b[0;32m   1231\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m         \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1234\u001b[0m         \u001b[0msubarr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type"
     ]
    }
   ],
   "source": [
    "#Hum----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_hum = SimpleImputer(\n",
    "input_columns=['month','freez', 'temp_avg', 'rain','wind_avg','rain_1mm','rain_cum','rain_max_10','rain_max_day'],\n",
    "output_column='hum',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_hum.fit(train_df=df_train)\n",
    "predictions_hum = imputer_hum.predict(df_test)\n",
    "\n",
    "pre_hum= predictions_hum.loc[~predictions_hum['hum'].isnull(),['hum','hum_imputed'] ]\n",
    "\n",
    "#Calculate f1 score\n",
    "r2_hum = r2_score(pre_hum['hum'], pre_hum['hum_imputed'])\n",
    "msq_hum = mean_squared_error(pre_hum['hum'], pre_hum['hum_imputed'])\n",
    "\n",
    "\n",
    "#completing hum data\n",
    "\n",
    "seasons_1= imputer_hum.predict(seasons.loc[seasons['hum'].isnull(),:])\n",
    "del seasons_1[\"hum\"]\n",
    "seasons_1=seasons_1.rename(columns={'hum_imputed':'hum'}).append(seasons.loc[~seasons['hum'].isnull(),:])\n",
    "\n",
    "\n",
    "#Freez----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test= random_split(seasons_1, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_freez = SimpleImputer(\n",
    "input_columns=['month','hum', 'temp_avg', 'rain','wind_avg'],\n",
    "output_column='freez',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_freez.fit(train_df=df_train)\n",
    "predictions_freez = imputer_freez.predict(df_test)\n",
    "\n",
    "pre_freez= predictions_freez.loc[~predictions_freez['freez'].isnull(),['freez','freez_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_freez = r2_score(pre_freez['freez'], pre_freez['freez_imputed'])\n",
    "msq_freez = mean_squared_error(pre_freez['freez'], pre_freez['freez_imputed'])\n",
    "\n",
    "seasons_2= imputer_freez.predict(seasons_1.loc[seasons_1['freez'].isnull(),:])\n",
    "del seasons_2[\"freez\"]\n",
    "seasons_2=seasons_2.rename(columns={'freez_imputed':'freez'}).append(seasons_1.loc[~seasons['freez'].isnull(),:])\n",
    "\n",
    "\n",
    "\n",
    "#Rain----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_2, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_rain = SimpleImputer(\n",
    "input_columns=['month','hum', 'temp_avg','wind_avg', 'freez','rain_1mm','rain_cum','rain_max_10','rain_max_day'],\n",
    "output_column='rain',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_rain.fit(train_df=df_train)\n",
    "predictions_rain = imputer_rain.predict(df_test)\n",
    "\n",
    "pre_rain= predictions_rain.loc[~predictions_rain['rain'].isnull(),['rain','rain_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_rain = r2_score(pre_rain['rain'], pre_rain['rain_imputed'])\n",
    "msq_rain = mean_squared_error(pre_rain['rain'], pre_rain['rain_imputed'])\n",
    "\n",
    "seasons_3= imputer_rain.predict(seasons_2.loc[seasons_2['rain'].isnull(),:])\n",
    "del seasons_3[\"rain\"]\n",
    "seasons_3=seasons_3.rename(columns={'rain_imputed':'rain'}).append(seasons_2.loc[~seasons['rain'].isnull(),:])\n",
    "\n",
    "\n",
    "#lev_max----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_3, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_lev_max = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','lev_mid','lev_min'],\n",
    "output_column='lev_max',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_lev_max.fit(train_df=df_train)\n",
    "predictions_lev_max = imputer_lev_max.predict(df_test)\n",
    "\n",
    "pre_lev_max= predictions_lev_max.loc[~predictions_lev_max['lev_max'].isnull(),['lev_max','lev_max_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_lev_max = r2_score(pre_lev_max['lev_max'], pre_lev_max['lev_max_imputed'])\n",
    "msq_lev_max = mean_squared_error(pre_lev_max['lev_max'], pre_lev_max['lev_max_imputed'])\n",
    "\n",
    "seasons_4= imputer_lev_max.predict(seasons_3.loc[seasons_3['lev_max'].isnull(),:])\n",
    "del seasons_4[\"lev_max\"]\n",
    "seasons_4=seasons_4.rename(columns={'lev_max_imputed':'lev_max'}).append(seasons_3.loc[~seasons['lev_max'].isnull(),:])\n",
    "\n",
    "\n",
    "#lev_mid----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_4, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_lev_mid = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','lev_min','lev_max'],\n",
    "output_column='lev_mid',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_lev_mid.fit(train_df=df_train)\n",
    "predictions_lev_mid = imputer_lev_mid.predict(df_test)\n",
    "\n",
    "pre_lev_mid= predictions_lev_mid.loc[~predictions_lev_mid['lev_mid'].isnull(),['lev_mid','lev_mid_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_lev_mid = r2_score(pre_lev_mid['lev_mid'], pre_lev_mid['lev_mid_imputed'])\n",
    "msq_lev_mid = mean_squared_error(pre_lev_mid['lev_mid'], pre_lev_mid['lev_mid_imputed'])\n",
    "\n",
    "seasons_5= imputer_lev_mid.predict(seasons_4.loc[seasons_4['lev_mid'].isnull(),:])\n",
    "del seasons_5[\"lev_mid\"]\n",
    "seasons_5=seasons_5.rename(columns={'lev_mid_imputed':'lev_mid'}).append(seasons_4.loc[~seasons['lev_mid'].isnull(),:])\n",
    "\n",
    "\n",
    "\n",
    "#lev_min----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_5, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_lev_min = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','lev_mid','lev_max'],\n",
    "output_column='lev_min',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_lev_min.fit(train_df=df_train)\n",
    "predictions_lev_min = imputer_lev_min.predict(df_test)\n",
    "\n",
    "pre_lev_min= predictions_lev_min.loc[~predictions_lev_min['lev_min'].isnull(),['lev_min','lev_min_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_lev_min = r2_score(pre_lev_min['lev_min'], pre_lev_min['lev_min_imputed'])\n",
    "msq_lev_min = mean_squared_error(pre_lev_min['lev_min'], pre_lev_min['lev_min_imputed'])\n",
    "\n",
    "seasons_6= imputer_lev_min.predict(seasons_5.loc[seasons_5['lev_min'].isnull(),:])\n",
    "del seasons_6[\"lev_min\"]\n",
    "seasons_6=seasons_6.rename(columns={'lev_min_imputed':'lev_min'}).append(seasons_5.loc[~seasons['lev_min'].isnull(),:])\n",
    "\n",
    "\n",
    "#rain_1mm----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_6, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_rain_1mmn = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','rain_cum','rain_max_10','rain_max_day'],\n",
    "output_column='rain_1mm',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_rain_1mmn.fit(train_df=df_train)\n",
    "predictions_rain_1mm = imputer_rain_1mmn.predict(df_test)\n",
    "\n",
    "pre_rain_1mm= predictions_rain_1mm.loc[~predictions_rain_1mm['rain_1mm'].isnull(),['rain_1mm','rain_1mm_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_rain_1mm = r2_score(pre_rain_1mm['rain_1mm'], pre_rain_1mm['rain_1mm_imputed'])\n",
    "msq_rain_1mm = mean_squared_error(pre_rain_1mm['rain_1mm'], pre_rain_1mm['rain_1mm_imputed'])\n",
    "\n",
    "seasons_7= imputer_rain_1mmn.predict(seasons_6.loc[seasons_6['rain_1mm'].isnull(),:])\n",
    "del seasons_7[\"rain_1mm\"]\n",
    "seasons_7=seasons_7.rename(columns={'rain_1mm_imputed':'rain_1mm'}).append(seasons_6.loc[~seasons['rain_1mm'].isnull(),:])\n",
    "\n",
    "\n",
    "#rain_cum ----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_7, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_rain_cum  = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'freez','sun','rain_1mm','rain_max_10','rain_max_day','lev_max','lev_mid','lev_min'],\n",
    "output_column='rain_cum',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_rain_cum.fit(train_df=df_train)\n",
    "predictions_rain_cum  = imputer_rain_cum.predict(df_test)\n",
    "\n",
    "pre_rain_cum = predictions_rain_cum.loc[~predictions_rain_cum ['rain_cum'].isnull(),['rain_cum','rain_cum_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_rain_cum = r2_score(pre_rain_cum['rain_cum'], pre_rain_cum['rain_cum_imputed'])\n",
    "msq_rain_cum = mean_squared_error(pre_rain_cum['rain_cum'], pre_rain_cum['rain_cum_imputed'])\n",
    "\n",
    "seasons_8= imputer_rain_cum.predict(seasons_7.loc[seasons_7['rain_cum'].isnull(),:])\n",
    "del seasons_8[\"rain_cum\"]\n",
    "seasons_8=seasons_8.rename(columns={'rain_cum_imputed':'rain_cum'}).append(seasons_7.loc[~seasons['rain_cum'].isnull(),:])\n",
    "\n",
    "#rain_max_10----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_8, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_rain_max_10 = SimpleImputer(\n",
    "input_columns=['hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','rain_cum','rain_1mm','rain_max_day'],\n",
    "output_column='rain_max_10',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_rain_max_10.fit(train_df=df_train)\n",
    "predictions_rain_max_10 = imputer_rain_max_10.predict(df_test)\n",
    "\n",
    "pre_rain_max_10= predictions_rain_max_10.loc[~predictions_rain_max_10['rain_max_10'].isnull(),['rain_max_10','rain_max_10_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_rain_max_10 = r2_score(pre_rain_max_10['rain_max_10'], pre_rain_max_10['rain_max_10_imputed'])\n",
    "msq_rain_max_10= mean_squared_error(pre_rain_max_10['rain_max_10'], pre_rain_max_10['rain_max_10_imputed'])\n",
    "\n",
    "seasons_9= imputer_rain_max_10.predict(seasons_8.loc[seasons_8['rain_max_10'].isnull(),:])\n",
    "del seasons_9[\"rain_max_10\"]\n",
    "seasons_9=seasons_9.rename(columns={'rain_max_10_imputed':'rain_max_10'}).append(seasons_8.loc[~seasons['rain_max_10'].isnull(),:])\n",
    "\n",
    "\n",
    "#rain_max_day----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_9, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_rain_max_day = SimpleImputer(\n",
    "input_columns=['month','hum', 'temp_avg','wind_avg', 'rain', 'freez','sun','rain_cum','rain_1mm','rain_max_10'],\n",
    "output_column='rain_max_day',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_rain_max_day.fit(train_df=df_train)\n",
    "predictions_rain_max_day = imputer_rain_max_day.predict(df_test)\n",
    "\n",
    "pre_rain_max_day =predictions_rain_max_day.loc[~predictions_rain_max_day['rain_max_day'].isnull(),['rain_max_day','rain_max_day_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_rain_max_day = r2_score(pre_rain_max_day['rain_max_day'], pre_rain_max_day['rain_max_day_imputed'])\n",
    "msq_rain_max_day= mean_squared_error(pre_rain_max_day['rain_max_day'], pre_rain_max_day['rain_max_day_imputed'])\n",
    "\n",
    "seasons_10= imputer_rain_max_day.predict(seasons_9.loc[seasons_9['rain_max_day'].isnull(),:])\n",
    "del seasons_10[\"rain_max_day\"]\n",
    "seasons_10=seasons_10.rename(columns={'rain_max_day_imputed':'rain_max_day'}).append(seasons_9.loc[~seasons['rain_max_10'].isnull(),:])\n",
    "\n",
    "\n",
    "\n",
    "#temp_avg----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_10, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_temp_avg = SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_max_abs','temp_max_avg','temp_min_abs', 'temp_min_avg'],\n",
    "output_column='temp_avg',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_temp_avg.fit(train_df=df_train)\n",
    "predictions_temp_avg = imputer_temp_avg.predict(df_test)\n",
    "\n",
    "pre_temp_avg =predictions_temp_avg.loc[~predictions_temp_avg['temp_avg'].isnull(),['temp_avg','temp_avg_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_temp_avg = r2_score(pre_temp_avg['temp_avg'], pre_temp_avg['temp_avg_imputed'])\n",
    "msq_temp_avg= mean_squared_error(pre_temp_avg['temp_avg'], pre_temp_avg['temp_avg_imputed'])\n",
    "\n",
    "seasons_11= imputer_temp_avg.predict(seasons_10.loc[seasons_10['temp_avg'].isnull(),:])\n",
    "del seasons_11[\"temp_avg\"]\n",
    "seasons_11=seasons_11.rename(columns={'temp_avg_imputed':'temp_avg'}).append(seasons_10.loc[~seasons['temp_avg'].isnull(),:])\n",
    "\n",
    "\n",
    "\n",
    "#temp_max_abs----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_11, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_temp_max_abs = SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_max_avg','temp_avg','temp_min_abs'],\n",
    "output_column='temp_max_abs',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_temp_max_abs.fit(train_df=df_train)\n",
    "predictions_temp_max_abs = imputer_temp_max_abs.predict(df_test)\n",
    "\n",
    "pre_temp_max_abs=predictions_temp_max_abs.loc[~predictions_temp_max_abs['temp_max_abs'].isnull(),['temp_max_abs','temp_max_abs_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_temp_max_abs= r2_score(pre_temp_max_abs['temp_max_abs'], pre_temp_max_abs['temp_max_abs_imputed'])\n",
    "msq_temp_max_abs= mean_squared_error(pre_temp_max_abs['temp_max_abs'], pre_temp_max_abs['temp_max_abs_imputed'])\n",
    "\n",
    "seasons_12= imputer_temp_max_abs.predict(seasons_11.loc[seasons_11['temp_max_abs'].isnull(),:])\n",
    "del seasons_12[\"temp_max_abs\"]\n",
    "seasons_12=seasons_12.rename(columns={'temp_max_abs_imputed':'temp_max_abs'}).append(seasons_11.loc[~seasons['temp_max_abs'].isnull(),:])\n",
    "\n",
    "\n",
    "#temp_max_avg----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_12, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_temp_max_avg = SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_max_abs','temp_avg','temp_min_abs'],\n",
    "output_column='temp_max_avg',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_temp_max_avg.fit(train_df=df_train)\n",
    "predictions_temp_max_avg= imputer_temp_max_avg.predict(df_test)\n",
    "\n",
    "pre_temp_max_avg=predictions_temp_max_avg.loc[~predictions_temp_max_avg['temp_max_avg'].isnull(),['temp_max_avg','temp_max_avg_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_temp_max_avg= r2_score(pre_temp_max_avg['temp_max_avg'], pre_temp_max_avg['temp_max_avg_imputed'])\n",
    "msq_temp_max_avg= mean_squared_error(pre_temp_max_avg['temp_max_avg'], pre_temp_max_avg['temp_max_avg_imputed'])\n",
    "\n",
    "seasons_13= imputer_temp_max_avg.predict(seasons_12.loc[seasons_12['temp_max_avg'].isnull(),:])\n",
    "del seasons_13[\"temp_max_avg\"]\n",
    "seasons_13=seasons_13.rename(columns={'temp_max_avg_imputed':'temp_max_avg'}).append(seasons_12.loc[~seasons['temp_max_avg'].isnull(),:])\n",
    "\n",
    "\n",
    "#temp_min_abs----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_13, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_temp_min_abs = SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_max_abs','temp_avg','temp_max_avg'],\n",
    "output_column='temp_min_abs',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_temp_min_abs.fit(train_df=df_train)\n",
    "predictions_temp_min_abs= imputer_temp_min_abs.predict(df_test)\n",
    "\n",
    "pre_temp_min_abs=predictions_temp_min_abs.loc[~predictions_temp_min_abs['temp_min_abs'].isnull(),['temp_min_abs','temp_min_abs_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_temp_min_abs= r2_score(pre_temp_min_abs['temp_min_abs'], pre_temp_min_abs['temp_min_abs_imputed'])\n",
    "msq_temp_min_abs= mean_squared_error(pre_temp_min_abs['temp_min_abs'], pre_temp_min_abs['temp_min_abs_imputed'])\n",
    "\n",
    "seasons_14= imputer_temp_min_abs.predict(seasons_13.loc[seasons_13['temp_min_abs'].isnull(),:])\n",
    "del seasons_14[\"temp_min_abs\"]\n",
    "seasons_14=seasons_14.rename(columns={'temp_min_abs_imputed':'temp_min_abs'}).append(seasons_13.loc[~seasons['temp_min_abs'].isnull(),:])\n",
    "\n",
    "\n",
    "#wind_avg----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_14, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_wind_avg = SimpleImputer(\n",
    "input_columns=['hum','wind_max', 'rain', 'freez','sun','temp_avg', 'wind_max_avg'],\n",
    "output_column='wind_avg',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_wind_avg.fit(train_df=df_train)\n",
    "predictions_wind_avg= imputer_wind_avg.predict(df_test)\n",
    "\n",
    "pre_wind_avg=predictions_wind_avg.loc[~predictions_wind_avg['wind_avg'].isnull(),['wind_avg','wind_avg_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_wind_avg= r2_score(pre_wind_avg['wind_avg'], pre_wind_avg['wind_avg_imputed'])\n",
    "msq_wind_avg= mean_squared_error(pre_wind_avg['wind_avg'], pre_wind_avg['wind_avg_imputed'])\n",
    "\n",
    "seasons_15= imputer_wind_avg.predict(seasons_14.loc[seasons_14['wind_avg'].isnull(),:])\n",
    "del seasons_15[\"wind_avg\"]\n",
    "seasons_15=seasons_15.rename(columns={'wind_avg_imputed':'wind_avg'}).append(seasons_14.loc[~seasons['wind_avg'].isnull(),:])\n",
    "\n",
    "\n",
    "#wind_max----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_15, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_wind_max = SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_avg', 'wind_max_avg'],\n",
    "output_column='wind_max',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_wind_max.fit(train_df=df_train)\n",
    "predictions_wind_max= imputer_wind_max.predict(df_test)\n",
    "\n",
    "pre_wind_max=predictions_wind_max.loc[~predictions_wind_max['wind_max'].isnull(),['wind_max','wind_max_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_wind_max= r2_score(pre_wind_max['wind_max'], pre_wind_max['wind_max_imputed'])\n",
    "msq_wind_max= mean_squared_error(pre_wind_max['wind_max'], pre_wind_max['wind_max_imputed'])\n",
    "\n",
    "seasons_16= imputer_wind_max.predict(seasons_15.loc[seasons_15['wind_max'].isnull(),:])\n",
    "del seasons_16[\"wind_max\"]\n",
    "seasons_16=seasons_16.rename(columns={'wind_max_imputed':'wind_max'}).append(seasons_15.loc[~seasons['wind_max'].isnull(),:])\n",
    "\n",
    "\n",
    "\n",
    "#wind_max_avg----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_16, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_wind_max_avg = SimpleImputer(\n",
    "input_columns=['hum','wind_max', 'rain', 'freez','sun','temp_avg', 'wind_max_avg'],\n",
    "output_column='wind_max_avg',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_wind_max_avg.fit(train_df=df_train)\n",
    "predictions_wind_max_avg= imputer_wind_max_avg.predict(df_test)\n",
    "\n",
    "pre_wind_max_avg=predictions_wind_max_avg.loc[~predictions_wind_max_avg['wind_max_avg'].isnull(),['wind_max_avg','wind_max_avg_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_wind_max_avg= r2_score(pre_wind_max_avg['wind_max_avg'], pre_wind_max_avg['wind_max_avg_imputed'])\n",
    "msq_wind_max_avg= mean_squared_error(pre_wind_max_avg['wind_max_avg'], pre_wind_max_avg['wind_max_avg_imputed'])\n",
    "\n",
    "seasons_17= imputer_wind_max_avg.predict(seasons_16.loc[seasons_16['wind_max_avg'].isnull(),:])\n",
    "del seasons_17[\"wind_max_avg\"]\n",
    "seasons_17=seasons_17.rename(columns={'wind_max_avg_imputed':'wind_max_avg'}).append(seasons_16.loc[~seasons['wind_max_avg'].isnull(),:])\n",
    "\n",
    "\n",
    "#sun----------------------------------------------------------------------------------------\n",
    "\n",
    "df_train, df_test = random_split(seasons_17, split_ratios=[0.8, 0.2])\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer_sun= SimpleImputer(\n",
    "input_columns=['hum','wind_avg', 'rain', 'freez','sun','temp_avg'],\n",
    "output_column='sun',\n",
    "output_path = 'imputer_model'\n",
    ")\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer_sun.fit(train_df=df_train)\n",
    "predictions_sun= imputer_sun.predict(df_test)\n",
    "\n",
    "pre_sun=predictions_sun.loc[~predictions_sun['sun'].isnull(),['sun','sun_imputed'] ]\n",
    "\n",
    "#Calculate R2 & MSE\n",
    "r2_sun= r2_score(pre_sun['sun'], pre_sun['sun_imputed'])\n",
    "msq_sun= mean_squared_error(pre_sun['sun'], pre_sun['sun_imputed'])\n",
    "\n",
    "seasons_18= imputer_sun.predict(seasons_17.loc[seasons_17['sun'].isnull(),:])\n",
    "del seasons_18[\"sun\"]\n",
    "seasons_18=seasons_18.rename(columns={'sun_imputed':'sun'}).append(seasons_17.loc[~seasons['sun'].isnull(),:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_data= seasons_18.loc[:,['codigo', ' estacion','year', 'month', 'hum', 'freez', 'rain', 'lev_max',\n",
    "       'lev_mid',  'lev_min','rain_1mm', 'rain_cum',  'rain_max_10','rain_max_day',\n",
    "       'temp_avg', 'temp_max_abs', 'temp_max_avg','temp_min_abs','wind_avg',\n",
    "       'wind_max', 'wind_max_avg','sun', ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasons_18.to_csv('seasons_impute.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imp_season= pd.read_csv('seasons_impute.csv')\n",
    "imp_season = seasons_18.copy()\n",
    "imp_season.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Generate the YEARLY dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017\n",
    "imp_season_anual_17= imp_season.loc[imp_season.year==2017,['codigo', 'freez', 'hum','lev_max', 'lev_mid', 'lev_min', 'rain',\n",
    "                                'rain_1mm', 'rain_cum', 'rain_max_10', 'rain_max_day', 'sun',\n",
    "                                'temp_avg', 'temp_max_abs', 'temp_max_avg', 'temp_min_abs', 'wind_avg','wind_max', 'wind_max_avg','year']]\n",
    "\n",
    "imp_season_anual_17=imp_season_anual_17.groupby(['codigo','year'],as_index=True ).agg({'freez':'mean', \n",
    "                         'hum':'mean', \n",
    "                         'lev_max':'max', \n",
    "                         'lev_mid':'mean',\n",
    "                         'lev_min':'min', \n",
    "                         'rain':'mean', \n",
    "                         'rain_1mm':'mean',\n",
    "                         'rain_cum':'mean', \n",
    "                         'rain_max_10':'max', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'sun':'mean',\n",
    "                         'temp_avg':'mean',\n",
    "                         'temp_max_abs':'max',\n",
    "                         'temp_max_avg':'max', \n",
    "                         'temp_min_abs':'min', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'wind_avg':'mean',\n",
    "                         'wind_max':'max',                                            \n",
    "                         'wind_max_avg':'max'}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "#2018\n",
    "imp_season_anual_18= imp_season.loc[imp_season.year==2018,['codigo', 'freez', 'hum','lev_max', 'lev_mid', 'lev_min', 'rain',\n",
    "                                'rain_1mm', 'rain_cum', 'rain_max_10', 'rain_max_day', 'sun',\n",
    "                                'temp_avg', 'temp_max_abs', 'temp_max_avg', 'temp_min_abs', 'wind_avg','wind_max', 'wind_max_avg','year']]\n",
    "\n",
    "imp_season_anual_18=imp_season_anual_18.groupby(['codigo','year'],as_index=True ).agg({'freez':'mean', \n",
    "                         'hum':'mean', \n",
    "                         'lev_max':'max', \n",
    "                         'lev_mid':'mean',\n",
    "                         'lev_min':'min', \n",
    "                         'rain':'mean', \n",
    "                         'rain_1mm':'mean',\n",
    "                         'rain_cum':'mean', \n",
    "                         'rain_max_10':'max', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'sun':'mean',\n",
    "                         'temp_avg':'mean',\n",
    "                         'temp_max_abs':'max',\n",
    "                         'temp_max_avg':'max', \n",
    "                         'temp_min_abs':'min', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'wind_avg':'mean',\n",
    "                         'wind_max':'max',                                            \n",
    "                         'wind_max_avg':'max'}).reset_index()\n",
    "\n",
    "#2019\n",
    "imp_season_anual_19= imp_season.loc[imp_season.year==2019,['codigo', 'freez', 'hum','lev_max', 'lev_mid', 'lev_min', 'rain',\n",
    "                                'rain_1mm', 'rain_cum', 'rain_max_10', 'rain_max_day', 'sun',\n",
    "                                'temp_avg', 'temp_max_abs', 'temp_max_avg', 'temp_min_abs', 'wind_avg','wind_max', 'wind_max_avg','year']]\n",
    "\n",
    "imp_season_anual_19=imp_season_anual_19.groupby(['codigo','year'],as_index=True ).agg({'freez':'mean', \n",
    "                         'hum':'mean', \n",
    "                         'lev_max':'max', \n",
    "                         'lev_mid':'mean',\n",
    "                         'lev_min':'min', \n",
    "                         'rain':'mean', \n",
    "                         'rain_1mm':'mean',\n",
    "                         'rain_cum':'mean', \n",
    "                         'rain_max_10':'max', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'sun':'mean',\n",
    "                         'temp_avg':'mean',\n",
    "                         'temp_max_abs':'max',\n",
    "                         'temp_max_avg':'max', \n",
    "                         'temp_min_abs':'min', \n",
    "                         'rain_max_day':'max',                         \n",
    "                         'wind_avg':'mean',\n",
    "                         'wind_max':'max',                                            \n",
    "                         'wind_max_avg':'max'}).reset_index()\n",
    "\n",
    "imp_season_anual= imp_season_anual_17.append(imp_season_anual_18).append(imp_season_anual_19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_season_anual.to_csv('WBds02_METEO.csv',index=False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
