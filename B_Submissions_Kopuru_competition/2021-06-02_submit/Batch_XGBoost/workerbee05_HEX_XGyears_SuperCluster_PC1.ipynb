{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/IEwaspbusters/KopuruVespaCompetitionIE/blob/main/Competition_subs/2021-04-28_submit/batch_LARVAE/HEX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Years: Prediction with Cluster Variables and selected Weather Variables (according to Feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data & Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rt-Jj2BjesTz",
    "outputId": "171cecde-0242-4aaf-82b8-0e3458dff994"
   },
   "outputs": [],
   "source": [
    "# Base packages -----------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Viz -----------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10) # to set figure size when ploting feature_importance\n",
    "\n",
    "\n",
    "# XGBoost -------------------------------\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance # built-in function to plot features ordered by their importance\n",
    "\n",
    "# SKLearn -----------------------------------------\n",
    "from sklearn import preprocessing # scaling data\n",
    "\n",
    "#Cluster\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function that checks if final Output is ready for submission or needs revision   \n",
    "\n",
    "def check_data(HEX):\n",
    "    \n",
    "    def template_checker(HEX):\n",
    "        submission_df = (HEX[\"CODIGO MUNICIPIO\"].astype(\"string\")+HEX[\"NOMBRE MUNICIPIO\"]).sort_values().reset_index(drop=True)\n",
    "        template_df = (template[\"CODIGO MUNICIPIO\"].astype(\"string\")+template[\"NOMBRE MUNICIPIO\"]).sort_values().reset_index(drop=True)\n",
    "        check_df = pd.DataFrame({\"submission_df\":submission_df,\"template_df\":template_df})\n",
    "        check_df[\"check\"] = check_df.submission_df == check_df.template_df\n",
    "        if (check_df.check == False).any():\n",
    "            pd.options.display.max_rows = 112\n",
    "            return check_df.loc[check_df.check == False,:]\n",
    "        else:  \n",
    "            return \"All Municipality Names and Codes to be submitted match the Template\"\n",
    "    \n",
    "    print(\"Submission form Shape is\", HEX.shape)\n",
    "    print(\"Number of Municipalities is\", HEX[\"CODIGO MUNICIPIO\"].nunique())\n",
    "    print(\"The Total 2020 Nests' Prediction is\", int(HEX[\"NIDOS 2020\"].sum()))\n",
    "\n",
    "    assert HEX.shape == (112, 3), \"Error: Shape is incorrect.\"\n",
    "    assert HEX[\"CODIGO MUNICIPIO\"].nunique() == 112, \"Error: Number of unique municipalities is correct.\"    \n",
    "    return template_checker(HEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9MLidG_FwhYB"
   },
   "outputs": [],
   "source": [
    "# Importing datasets from GitHub as Pandas Dataframes\n",
    "queen_train = pd.read_csv(\"../Feeder_years/WBds03_QUEENtrainYEARS.csv\", encoding=\"utf-8\") #2018+2019 test df\n",
    "queen_predict = pd.read_csv(\"../Feeder_years/WBds03_QUEENpredictYEARS.csv\", encoding=\"utf-8\") #2020 prediction df\n",
    "template = pd.read_csv(\"../../../Input_open_data/ds01_PLANTILLA-RETO-AVISPAS-KOPURU.csv\",sep=\";\", encoding=\"utf-8\")\n",
    "den_com = pd.read_excel(\"../../../Other_open_data/densidad comercial.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New queen Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "den_com_18= den_com.loc[:,['Código municipio','2018']]\n",
    "den_com_18.rename({'2018': 'dens_com','Código municipio':'municip_code'}, axis=1, inplace=True)\n",
    "den_com_18['year_offset']='2018'\n",
    "\n",
    "den_com_17= den_com.loc[:,['Código municipio','2017']]\n",
    "den_com_17.rename({'2017': 'dens_com','Código municipio':'municip_code'}, axis=1, inplace=True)\n",
    "den_com_17['year_offset']='2017'\n",
    "\n",
    "den_com_19= den_com.loc[:,['Código municipio','2019']]\n",
    "den_com_19.rename({'2019': 'dens_com','Código municipio':'municip_code'}, axis=1, inplace=True)\n",
    "den_com_19['year_offset']='2019'\n",
    "\n",
    "densidad_comercial= den_com_18.append(den_com_17).append(den_com_19)\n",
    "densidad_comercial['cod_aux']=densidad_comercial.apply(lambda x:'%s_%s' % (x['municip_code'],x['year_offset']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f16408cc89f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m queen_train_mischief= aux_train.loc[:,\n\u001b[0;32m      5\u001b[0m                                     ['municip_code','municip_name','weath_meanTemp',\n\u001b[1;32m----> 6\u001b[1;33m                                      'population','cod_aux','NESTS']].merge(cluster_data,\n\u001b[0m\u001b[0;32m      7\u001b[0m                                                                             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'municip_code'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdensidad_comercial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                                                                                                  how='left', on='cod_aux') \n",
      "\u001b[1;31mNameError\u001b[0m: name 'cluster_data' is not defined"
     ]
    }
   ],
   "source": [
    "aux_train= queen_train.copy()\n",
    "aux_train['cod_aux']=aux_train.apply(lambda x:'%s_%s' % (x['municip_code'],x['year_offset']),axis=1)\n",
    "\n",
    "queen_train_mischief= aux_train.loc[:,\n",
    "                                    ['municip_code','municip_name','weath_meanTemp',\n",
    "                                     'population','cod_aux','NESTS']].merge(cluster_data,\n",
    "                                                                            how='left', on='municip_code').merge(densidad_comercial,\n",
    "                                                                                                                 how='left', on='cod_aux') \n",
    "\n",
    "                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "queen_train_mischief.drop(['cod_aux','municip_code_y'], axis=1, inplace=True)\n",
    "queen_train_mischief.rename({'municip_code_x': 'municip_code'}, axis=1, inplace=True)\n",
    "queen_train_mischief[\"dens_com\"] = queen_train_mischief[\"dens_com\"].apply(lambda x: x.replace(\",\", \".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queen_predict_mischief= queen_predict.loc[:,['municip_code','municip_name','weath_meanTemp','year_offset','population']]\n",
    "queen_predict_mischief['cod_aux']= queen_predict_mischief.apply(lambda x:'%s_%s' % (x['municip_code'],x['year_offset']),axis=1)\n",
    "queen_predict_mischief= queen_predict_mischief.merge(densidad_comercial, how='left', on='cod_aux')\n",
    "queen_predict_mischief.drop(['cod_aux','municip_code_y','year_offset_x','year_offset_y'], axis=1, inplace=True)\n",
    "queen_predict_mischief.rename({'municip_code_x': 'municip_code'}, axis=1, inplace=True)\n",
    "queen_predict_mischief[\"dens_com\"] = queen_predict_mischief[\"dens_com\"].apply(lambda x: x.replace(\",\", \".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_20=queen_predict_mischief.loc[:,['weath_meanTemp', 'population', 'Cluster', 'dens_com']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_cluster= queen_predict.drop(['population','station_code'], axis=1).iloc[:,3:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cluster = preprocessing.minmax_scale(aux_cluster) # this creates a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.99)\n",
    "pca.fit(aux_cluster)\n",
    "aux_dim= pca.transform(aux_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silouhette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = []\n",
    "\n",
    "for i in range(2,10,1):\n",
    "  model = KMeans(n_clusters=i)\n",
    "  model.fit(aux_dim)\n",
    "  labels = model.labels_\n",
    "  sol = silhouette_score(aux_dim, labels)\n",
    "  silhouettes.append(sol)\n",
    "\n",
    "silhouette = pd.DataFrame()\n",
    "silhouette['Labels'] = silhouettes\n",
    "silhouette['NumberOfClusters'] = range(2,10,1)\n",
    "\n",
    "ggplot(aes(x='NumberOfClusters', y='Labels'), silhouette) + geom_line() + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_output = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we decided to seek out 2 clusters, as per de Silhouette method\n",
    "model = KMeans(n_clusters = silhouette_output)\n",
    "\n",
    "model.fit(aux_dim)\n",
    "modelLabels = model.labels_\n",
    "modelCenters = model.cluster_centers_\n",
    "\n",
    "queen_predict_mischief['Cluster'] = pd.Series(modelLabels, index=queen_predict_mischief.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data=queen_predict_mischief.loc[:,['municip_code', 'Cluster']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange data into a features matrix and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the train X & y variables\n",
    "\n",
    "# Y will be the response variable (filter for the number of wasp nests - waspbust_id)  \n",
    "y = queen_train_mischief.NESTS\n",
    "\n",
    "# X will be the explanatory variables. Remove response variable and non desired categorical columns such as (municip code, year, etc...)\n",
    "X = queen_train_mischief.loc[:,['weath_meanTemp', 'population', 'Cluster', 'dens_com']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Data in order to filter the relevant variables using Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrange data into a features matrix and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the datasets using MinMaxScaler\n",
    "\n",
    "X_scaled = preprocessing.minmax_scale(X) # this creates a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a class of model by importing the appropriate estimator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the XGBoost model and fitting with the train data\n",
    "model = XGBRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model to your data by calling the `.fit()` method of the model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the XGBoost model and fitting with the train data for each cluster\n",
    "\n",
    "model.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the Relevant Variables and filtering according to the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the Relevant Variables in order to filter the relevant ones per Cluster\n",
    "\n",
    "plot_importance(model,height=0.5,xlabel=\"F-Score\",ylabel=\"Feature Importance\",grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to your data by calling the `.fit()` method of the model instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the model to new data:\n",
    "\n",
    "- For supervised learning, predict labels for unknown data using the `.predict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "X_scaled_pred = preprocessing.minmax_scale(predict_20)\n",
    "queen_predict_mischief['nests_2020'] = model.predict(X_scaled_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queen_predict_mischief.nests_2020.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queen_predict_mischief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Each Cluster Predictions to the original DataFrame and Save it as a `.csv file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "Z3PcQ4UnACCA"
   },
   "outputs": [],
   "source": [
    "HEX = queen_predict_mischief.loc[:,[\"municip_code\",\"municip_name\",\"nests_2020\"]].round() # create a new Dataframe for Kopuru submission\n",
    "HEX.columns = [\"CODIGO MUNICIPIO\",\"NOMBRE MUNICIPIO\",\"NIDOS 2020\"] # change column names to Spanish (Decidata template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission form Shape is (112, 3)\n",
      "Number of Municipalities is 112\n",
      "The Total 2020 Nests' Prediction is 2851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All Municipality Names and Codes to be submitted match the Template'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final check\n",
    "\n",
    "check_data(HEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset max_rows to default values (used in function to see which rows did not match template)\n",
    "\n",
    "pd.reset_option(\"max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "uiPq7zXi0STt"
   },
   "outputs": [],
   "source": [
    "# Save the new dataFrame as a .csv in the current working directory on Windows\n",
    "\n",
    "HEX.to_csv(\"WaspBusters_20210602_XGyears_SuperCluster_PC1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "HEX draft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
