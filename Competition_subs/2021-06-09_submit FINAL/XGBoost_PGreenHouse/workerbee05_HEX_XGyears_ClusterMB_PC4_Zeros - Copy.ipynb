{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/IEwaspbusters/KopuruVespaCompetitionIE/blob/main/Competition_subs/2021-04-28_submit/batch_LARVAE/HEX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Years: Prediction with Cluster Variables and selected Weather Variables (according to Feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data & Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rt-Jj2BjesTz",
    "outputId": "171cecde-0242-4aaf-82b8-0e3458dff994"
   },
   "outputs": [],
   "source": [
    "# Base packages -----------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Viz -----------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10) # to set figure size when ploting feature_importance\n",
    "\n",
    "\n",
    "# XGBoost -------------------------------\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance # built-in function to plot features ordered by their importance\n",
    "\n",
    "# SKLearn -----------------------------------------\n",
    "from sklearn import preprocessing # scaling data\n",
    "\n",
    "#Cluster\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function that checks if final Output is ready for submission or needs revision   \n",
    "\n",
    "def check_data(HEX):\n",
    "    \n",
    "    def template_checker(HEX):\n",
    "        submission_df = (HEX[\"CODIGO MUNICIPIO\"].astype(\"string\")+HEX[\"NOMBRE MUNICIPIO\"]).sort_values().reset_index(drop=True)\n",
    "        template_df = (template[\"CODIGO MUNICIPIO\"].astype(\"string\")+template[\"NOMBRE MUNICIPIO\"]).sort_values().reset_index(drop=True)\n",
    "        check_df = pd.DataFrame({\"submission_df\":submission_df,\"template_df\":template_df})\n",
    "        check_df[\"check\"] = check_df.submission_df == check_df.template_df\n",
    "        if (check_df.check == False).any():\n",
    "            pd.options.display.max_rows = 112\n",
    "            return check_df.loc[check_df.check == False,:]\n",
    "        else:  \n",
    "            return \"All Municipality Names and Codes to be submitted match the Template\"\n",
    "    \n",
    "    print(\"Submission form Shape is\", HEX.shape)\n",
    "    print(\"Number of Municipalities is\", HEX[\"CODIGO MUNICIPIO\"].nunique())\n",
    "    print(\"The Total 2020 Nests' Prediction is\", int(HEX[\"NIDOS 2020\"].sum()))\n",
    "\n",
    "    assert HEX.shape == (112, 3), \"Error: Shape is incorrect.\"\n",
    "    assert HEX[\"CODIGO MUNICIPIO\"].nunique() == 112, \"Error: Number of unique municipalities is correct.\"    \n",
    "    return template_checker(HEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9MLidG_FwhYB"
   },
   "outputs": [],
   "source": [
    "# Importing datasets from GitHub as Pandas Dataframes\n",
    "queen_train = pd.read_csv(\"../Feeder_years/WBds03_QUEENtrainYEARS.csv\", encoding=\"utf-8\") #2018+2019 test df\n",
    "queen_predict = pd.read_csv(\"../Feeder_years/WBds03_QUEENpredictYEARS.csv\", encoding=\"utf-8\") #2020 prediction df\n",
    "template = pd.read_csv(\"../../../Input_open_data/ds01_PLANTILLA-RETO-AVISPAS-KOPURU.csv\",sep=\";\", encoding=\"utf-8\")\n",
    "den_com = pd.read_excel(\"../../../Other_open_data/densidad comercial.xlsx\")\n",
    "cluster= pd.read_csv(\"../Feeder_years/WBds_CLUSTERSnests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "den_com_melt= pd.melt(den_com, id_vars=['C칩digo municipio'], value_vars=['2019', '2018', '2017'], var_name='year_offset', \n",
    "                      value_name='densidad')\n",
    "den_com_melt.rename({'C칩digo municipio':'municip_code'}, axis=1, inplace=True)\n",
    "den_com_melt[\"densidad\"] = den_com_melt[\"densidad\"].apply(lambda x: x.replace(\",\", \".\"))\n",
    "den_com_melt['year_offset']= den_com_melt['year_offset'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New queen Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= queen_train.iloc[:,:33]\n",
    "df_train['year_offset']= df_train['year_offset'].apply(str)\n",
    "\n",
    "df_train = df_train.merge(den_com_melt,\\\n",
    "             how='left', left_on=['municip_code','year_offset'],\\\n",
    "             right_on=['municip_code','year_offset']).merge(cluster, how='left', on= 'municip_code') #Merge Densidad comercial + Cluster\n",
    "\n",
    "#Cleaning\n",
    "df_train.drop(['municip_name_y','station_code'], axis=1, inplace=True)\n",
    "df_train.rename({'municip_name_x': 'municip_name'}, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New queen predict dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queen_predict['year_offset']= queen_predict['year_offset'].apply(str)\n",
    "\n",
    "df_predict= queen_predict.loc[:,['municip_name', 'municip_code', 'year_offset','population']].merge(den_com_melt,\\\n",
    "             how='left', left_on=['municip_code','year_offset'],\\\n",
    "             right_on=['municip_code','year_offset']).merge(cluster, how='left',on='municip_code')\n",
    "\n",
    "df_predict.drop(['municip_name_y'], axis=1, inplace=True)\n",
    "df_predict.rename({'municip_name_x': 'municip_name'}, axis=1, inplace=True)\n",
    "\n",
    "#Aux to predict (X_Predict)\n",
    "aux_predict= df_predict.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train.NESTS\n",
    "\n",
    "# X will be the explanatory variables. Remove response variable and non desired categorical columns such as (municip code, year, etc...)\n",
    "X = df_train.loc[:,['population', 'densidad','Cluster']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the datasets using MinMaxScaler\n",
    "\n",
    "X_scaled = preprocessing.minmax_scale(X) # this creates a numpy array\n",
    "X_scaled = pd.DataFrame(X_scaled,index=X.index,columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the XGBoost model and fitting with the train data\n",
    "model = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor()"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting the XGBoost model and fitting with the train data for each cluster\n",
    "\n",
    "model.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "X_scaled_pred = preprocessing.minmax_scale(aux_predict)\n",
    "X_scaled_pred = pd.DataFrame(X_scaled_pred,index=aux_predict.index,columns=aux_predict.columns)\n",
    "df_predict['nests_2020'] = model.predict(X_scaled_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Each Cluster Predictions to the original DataFrame and Save it as a `.csv file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Municipalities to which we did not assign a Cluster, since there was not reliable data for us to predict \n",
    "\n",
    "df_predict = df_predict.loc[~df_predict.municip_code.isin([48071, 48074, 48022, 48088, 48051, 48020]),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the Municipalities to insert manualy\n",
    "\n",
    "HEX_aux = pd.DataFrame({\"CODIGO MUNICIPIO\":[48022, 48071, 48088, 48074, 48051, 48020],\\\n",
    "             \"NOMBRE MUNICIPIO\":[\"Karrantza Harana/Valle de Carranza\",\"Muskiz\",\"Ubide\",\"Urdu침a/Ordu침a\",\"Lanestosa\",\"Bilbao\"],\\\n",
    "             \"NIDOS 2020\":[0,0,1,0,1,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEX = df_predict.loc[:,[\"municip_code\",\"municip_name\",\"nests_2020\"]].round() # create a new Dataframe for Kopuru submission\n",
    "HEX.columns = [\"CODIGO MUNICIPIO\",\"NOMBRE MUNICIPIO\",\"NIDOS 2020\"] # change column names to Spanish (Decidata template)\n",
    "HEX = HEX.append(HEX_aux, ignore_index=True) # Add rows of municipalities to add manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission form Shape is (112, 3)\n",
      "Number of Municipalities is 112\n",
      "The Total 2020 Nests' Prediction is 2801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All Municipality Names and Codes to be submitted match the Template'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final check\n",
    "\n",
    "check_data(HEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset max_rows to default values (used in function to see which rows did not match template)\n",
    "\n",
    "pd.reset_option(\"max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "uiPq7zXi0STt"
   },
   "outputs": [],
   "source": [
    "# Save the new dataFrame as a .csv in the current working directory on Windows\n",
    "\n",
    "HEX.to_csv(\"WaspBusters_20210608_XGyears_ClusterMB_PC4_Zeros.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "HEX draft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
